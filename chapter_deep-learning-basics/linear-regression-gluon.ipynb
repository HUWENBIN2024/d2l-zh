{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归的简洁实现\n",
    "\n",
    "随着深度学习框架的发展，开发深度学习应用变得越来越便利。实践中，我们通常可以用比上一节更简洁的代码来实现同样的模型。在本节中，我们将介绍如何使用MXNet提供的Gluon接口更方便地实现线性回归的训练。\n",
    "\n",
    "## 生成数据集\n",
    "\n",
    "我们生成与上一节中相同的数据集。其中`features`是训练数据特征，`labels`是标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd, nd\n",
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "features = nd.random.normal(scale=1, shape=(num_examples, num_inputs))\n",
    "labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n",
    "labels += nd.random.normal(scale=0.01, shape=labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据集\n",
    "\n",
    "Gluon提供了`data`包来读取数据。由于`data`常用作变量名，我们将导入的`data`模块用添加了Gluon首字母的假名`gdata`代替。在每一次迭代中，我们将随机读取包含10个数据样本的小批量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import data as gdata\n",
    "batch_size = 10\n",
    "# 将训练数据的特征和标签组合\n",
    "dataset = gdata.ArrayDataset(features, labels)\n",
    "# 随机读取小批量\n",
    "data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里`data_iter`的使用与上一节中的一样。让我们读取并打印第一个小批量数据样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "for X, y in data_iter:\n",
    "    print(X, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "在上一节从零开始的实现中，我们需要定义模型参数，并使用它们一步步描述模型是怎样计算的。当模型结构变得更复杂时，这些步骤将变得更烦琐。其实，Gluon提供了大量预定义的层，这使我们只需关注使用哪些层来构造模型。下面将介绍如何使用Gluon更简洁地定义线性回归。\n",
    "\n",
    "首先，导入`nn`模块。实际上，“nn”是neural networks（神经网络）的缩写。顾名思义，该模块定义了大量神经网络的层。我们先定义一个模型变量`net`，它是一个`Sequential`实例。在Gluon中，`Sequential`实例可以看作是一个串联各个层的容器。在构造模型时，我们在该容器中依次添加层。当给定输入数据时，容器中的每一层将依次计算并将输出作为下一层的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "\n",
    "net = nn.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回顾图3.1中线性回归在神经网络图中的表示。作为一个单层神经网络，线性回归输出层中的神经元和输入层中各个输入完全连接。因此，线性回归的输出层又叫全连接层。在Gluon中，全连接层是一个`Dense`实例。我们定义该层输出个数为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "net.add(nn.Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得一提的是，在Gluon中我们无须指定每一层输入的形状，例如线性回归的输入个数。当模型得到数据时，例如后面执行`net(X)`时，模型将自动推断出每一层的输入个数。我们将在之后“深度学习计算”一章详细介绍这种机制。Gluon的这一设计为模型开发带来便利。\n",
    "\n",
    "\n",
    "## 初始化模型参数\n",
    "\n",
    "在使用`net`前，我们需要初始化模型参数，如线性回归模型中的权重和偏差。我们从MXNet导入`init`模块。该模块提供了模型参数初始化的各种方法。这里的`init`是`initializer`的缩写形式。我们通过`init.Normal(sigma=0.01)`指定权重参数每个元素将在初始化时随机采样于均值为0、标准差为0.01的正态分布。偏差参数默认会初始化为零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import init\n",
    "\n",
    "net.initialize(init.Normal(sigma=0.01)) \n",
    "# initialize()  defined by the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义损失函数\n",
    "\n",
    "在Gluon中，`loss`模块定义了各种损失函数。我们用假名`gloss`代替导入的`loss`模块，并直接使用它提供的平方损失作为模型的损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import loss as gloss\n",
    "\n",
    "loss = gloss.L2Loss()  # 平方损失又称L2范数损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义优化算法\n",
    "\n",
    "同样，我们也无须实现小批量随机梯度下降。在导入Gluon后，我们创建一个`Trainer`实例，并指定学习率为0.03的小批量随机梯度下降（`sgd`）为优化算法。该优化算法将用来迭代`net`实例所有通过`add`函数嵌套的层所包含的全部参数。这些参数可以通过`collect_params`函数获取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "\n",
    "在使用Gluon训练模型时，我们通过调用`Trainer`实例的`step`函数来迭代模型参数。上一节中我们提到，由于变量`l`是长度为`batch_size`的一维`NDArray`，执行`l.backward()`等价于执行`l.sum().backward()`。按照小批量随机梯度下降的定义，我们在`step`函数中指明批量大小，从而对批量中样本梯度求平均。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        with autograd.record():\n",
    "            l = loss(net(X), y)\n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "    l = loss(net(features), labels)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们分别比较学到的模型参数和真实的模型参数。我们从`net`获得需要的层，并访问其权重（`weight`）和偏差（`bias`）。学到的参数和真实的参数很接近。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [],
   "source": [
    "dense = net[0]\n",
    "true_w, dense.weight.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.2,\n",
       " \n",
       " [4.199861]\n",
       " <NDArray 1 @cpu(0)>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_b, dense.bias.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 使用Gluon可以更简洁地实现模型。\n",
    "* 在Gluon中，`data`模块提供了有关数据处理的工具，`nn`模块定义了大量神经网络的层，`loss`模块定义了各种损失函数。\n",
    "* MXNet的`initializer`模块提供了模型参数初始化的各种方法。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 如果将`l = loss(net(X), y)`替换成`l = loss(net(X), y).mean()`，我们需要将`trainer.step(batch_size)`相应地改成`trainer.step(1)`。这是为什么呢？\n",
    "* 查阅MXNet文档，看看`gluon.loss`和`init`模块里提供了哪些损失函数和初始化方法。\n",
    "* 如何访问`dense.weight`的梯度？\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 扫码直达[讨论区](https://discuss.gluon.ai/t/topic/742)\n",
    "\n",
    "![](../img/qr_linear-regression-gluon.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Dense in module mxnet.gluon.nn.basic_layers object:\n",
      "\n",
      "class Dense(mxnet.gluon.block.HybridBlock)\n",
      " |  Just your regular densely-connected NN layer.\n",
      " |  \n",
      " |  `Dense` implements the operation:\n",
      " |  `output = activation(dot(input, weight) + bias)`\n",
      " |  where `activation` is the element-wise activation function\n",
      " |  passed as the `activation` argument, `weight` is a weights matrix\n",
      " |  created by the layer, and `bias` is a bias vector created by the layer\n",
      " |  (only applicable if `use_bias` is `True`).\n",
      " |  \n",
      " |  Note: the input must be a tensor with rank 2. Use `flatten` to convert it\n",
      " |  to rank 2 manually if necessary.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  units : int\n",
      " |      Dimensionality of the output space.\n",
      " |  activation : str\n",
      " |      Activation function to use. See help on `Activation` layer.\n",
      " |      If you don't specify anything, no activation is applied\n",
      " |      (ie. \"linear\" activation: `a(x) = x`).\n",
      " |  use_bias : bool, default True\n",
      " |      Whether the layer uses a bias vector.\n",
      " |  flatten: bool, default True\n",
      " |      Whether the input tensor should be flattened.\n",
      " |      If true, all but the first axis of input data are collapsed together.\n",
      " |      If false, all but the last axis of input data are kept the same, and the transformation\n",
      " |      applies on the last axis.\n",
      " |  dtype : str or np.dtype, default 'float32'\n",
      " |      Data type of output embeddings.\n",
      " |  weight_initializer : str or `Initializer`\n",
      " |      Initializer for the `kernel` weights matrix.\n",
      " |  bias_initializer: str or `Initializer`\n",
      " |      Initializer for the bias vector.\n",
      " |  in_units : int, optional\n",
      " |      Size of the input data. If not specified, initialization will be\n",
      " |      deferred to the first time `forward` is called and `in_units`\n",
      " |      will be inferred from the shape of input data.\n",
      " |  prefix : str or None\n",
      " |      See document of `Block`.\n",
      " |  params : ParameterDict or None\n",
      " |      See document of `Block`.\n",
      " |  \n",
      " |  \n",
      " |  Inputs:\n",
      " |      - **data**: if `flatten` is True, `data` should be a tensor with shape\n",
      " |        `(batch_size, x1, x2, ..., xn)`, where x1 * x2 * ... * xn is equal to\n",
      " |        `in_units`. If `flatten` is False, `data` should have shape\n",
      " |        `(x1, x2, ..., xn, in_units)`.\n",
      " |  \n",
      " |  Outputs:\n",
      " |      - **out**: if `flatten` is True, `out` will be a tensor with shape\n",
      " |        `(batch_size, units)`. If `flatten` is False, `out` will have shape\n",
      " |        `(x1, x2, ..., xn, units)`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dense\n",
      " |      mxnet.gluon.block.HybridBlock\n",
      " |      mxnet.gluon.block.Block\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, units, activation=None, use_bias=True, flatten=True, dtype='float32', weight_initializer=None, bias_initializer='zeros', in_units=0, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  hybrid_forward(self, F, x, weight, bias=None)\n",
      " |      Overrides to construct symbolic graph for this `Block`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : Symbol or NDArray\n",
      " |          The first input tensor.\n",
      " |      *args : list of Symbol or list of NDArray\n",
      " |          Additional input tensors.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from mxnet.gluon.block.HybridBlock:\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Registers parameters.\n",
      " |  \n",
      " |  cast(self, dtype)\n",
      " |      Cast this Block to use another data type.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dtype : str or numpy.dtype\n",
      " |          The new data type.\n",
      " |  \n",
      " |  export(self, path, epoch=0, remove_amp_cast=True)\n",
      " |      Export HybridBlock to json format that can be loaded by\n",
      " |      `SymbolBlock.imports`, `mxnet.mod.Module` or the C++ interface.\n",
      " |      \n",
      " |      .. note:: When there are only one input, it will have name `data`. When there\n",
      " |                Are more than one inputs, they will be named as `data0`, `data1`, etc.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          Path to save model. Two files `path-symbol.json` and `path-xxxx.params`\n",
      " |          will be created, where xxxx is the 4 digits epoch number.\n",
      " |      epoch : int\n",
      " |          Epoch number of saved model.\n",
      " |  \n",
      " |  forward(self, x, *args)\n",
      " |      Defines the forward computation. Arguments can be either\n",
      " |      :py:class:`NDArray` or :py:class:`Symbol`.\n",
      " |  \n",
      " |  hybridize(self, active=True, **kwargs)\n",
      " |      Activates or deactivates :py:class:`HybridBlock` s recursively. Has no effect on\n",
      " |      non-hybrid children.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      active : bool, default True\n",
      " |          Whether to turn hybrid on or off.\n",
      " |      static_alloc : bool, default False\n",
      " |          Statically allocate memory to improve speed. Memory usage may increase.\n",
      " |      static_shape : bool, default False\n",
      " |          Optimize for invariant input shapes between iterations. Must also\n",
      " |          set static_alloc to True. Change of input shapes is still allowed\n",
      " |          but slower.\n",
      " |  \n",
      " |  infer_shape(self, *args)\n",
      " |      Infers shape of Parameters from inputs.\n",
      " |  \n",
      " |  infer_type(self, *args)\n",
      " |      Infers data type of Parameters from inputs.\n",
      " |  \n",
      " |  register_child(self, block, name=None)\n",
      " |      Registers block as a child of self. :py:class:`Block` s assigned to self as\n",
      " |      attributes will be registered automatically.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from mxnet.gluon.block.Block:\n",
      " |  \n",
      " |  __call__(self, *args)\n",
      " |      Calls forward. Only accepts positional arguments.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every child block as well as self.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fn : callable\n",
      " |          Function to be applied to each submodule, of form `fn(block)`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      this block\n",
      " |  \n",
      " |  collect_params(self, select=None)\n",
      " |      Returns a :py:class:`ParameterDict` containing this :py:class:`Block` and all of its\n",
      " |      children's Parameters(default), also can returns the select :py:class:`ParameterDict`\n",
      " |      which match some given regular expressions.\n",
      " |      \n",
      " |      For example, collect the specified parameters in ['conv1_weight', 'conv1_bias', 'fc_weight',\n",
      " |      'fc_bias']::\n",
      " |      \n",
      " |          model.collect_params('conv1_weight|conv1_bias|fc_weight|fc_bias')\n",
      " |      \n",
      " |      or collect all parameters whose names end with 'weight' or 'bias', this can be done\n",
      " |      using regular expressions::\n",
      " |      \n",
      " |          model.collect_params('.*weight|.*bias')\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      select : str\n",
      " |          regular expressions\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      The selected :py:class:`ParameterDict`\n",
      " |  \n",
      " |  initialize(self, init=<mxnet.initializer.Uniform object at 0x0000019440EB7208>, ctx=None, verbose=False, force_reinit=False)\n",
      " |      Initializes :py:class:`Parameter` s of this :py:class:`Block` and its children.\n",
      " |      Equivalent to ``block.collect_params().initialize(...)``\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      init : Initializer\n",
      " |          Global default Initializer to be used when :py:meth:`Parameter.init` is ``None``.\n",
      " |          Otherwise, :py:meth:`Parameter.init` takes precedence.\n",
      " |      ctx : Context or list of Context\n",
      " |          Keeps a copy of Parameters on one or many context(s).\n",
      " |      verbose : bool, default False\n",
      " |          Whether to verbosely print out details on initialization.\n",
      " |      force_reinit : bool, default False\n",
      " |          Whether to force re-initialization if parameter is already initialized.\n",
      " |  \n",
      " |  load_parameters(self, filename, ctx=None, allow_missing=False, ignore_extra=False, cast_dtype=False, dtype_source='current')\n",
      " |      Load parameters from file previously saved by `save_parameters`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      filename : str\n",
      " |          Path to parameter file.\n",
      " |      ctx : Context or list of Context, default cpu()\n",
      " |          Context(s) to initialize loaded parameters on.\n",
      " |      allow_missing : bool, default False\n",
      " |          Whether to silently skip loading parameters not represents in the file.\n",
      " |      ignore_extra : bool, default False\n",
      " |          Whether to silently ignore parameters from the file that are not\n",
      " |          present in this Block.\n",
      " |      cast_dtype : bool, default False\n",
      " |          Cast the data type of the NDArray loaded from the checkpoint to the dtype\n",
      " |          provided by the Parameter if any.\n",
      " |      dtype_source : str, default 'current'\n",
      " |          must be in {'current', 'saved'}\n",
      " |          Only valid if cast_dtype=True, specify the source of the dtype for casting\n",
      " |          the parameters\n",
      " |      References\n",
      " |      ----------\n",
      " |      `Saving and Loading Gluon Models         <https://mxnet.incubator.apache.org/tutorials/gluon/save_load_params.html>`_\n",
      " |  \n",
      " |  load_params(self, filename, ctx=None, allow_missing=False, ignore_extra=False)\n",
      " |      [Deprecated] Please use load_parameters.\n",
      " |      \n",
      " |      Load parameters from file.\n",
      " |      \n",
      " |      filename : str\n",
      " |          Path to parameter file.\n",
      " |      ctx : Context or list of Context, default cpu()\n",
      " |          Context(s) to initialize loaded parameters on.\n",
      " |      allow_missing : bool, default False\n",
      " |          Whether to silently skip loading parameters not represents in the file.\n",
      " |      ignore_extra : bool, default False\n",
      " |          Whether to silently ignore parameters from the file that are not\n",
      " |          present in this Block.\n",
      " |  \n",
      " |  name_scope(self)\n",
      " |      Returns a name space object managing a child :py:class:`Block` and parameter\n",
      " |      names. Should be used within a ``with`` statement::\n",
      " |      \n",
      " |          with self.name_scope():\n",
      " |              self.dense = nn.Dense(20)\n",
      " |      \n",
      " |      Please refer to\n",
      " |      `naming tutorial <http://mxnet.incubator.apache.org/tutorials/gluon/naming.html>`_\n",
      " |      for more info on prefix and naming.\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the block.\n",
      " |      \n",
      " |      The hook function is called immediately after :func:`forward`.\n",
      " |      It should not modify the input or output.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      hook : callable\n",
      " |          The forward hook function of form `hook(block, input, output) -> None`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`mxnet.gluon.utils.HookHandle`\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the block.\n",
      " |      \n",
      " |      The hook function is called immediately before :func:`forward`.\n",
      " |      It should not modify the input or output.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      hook : callable\n",
      " |          The forward hook function of form `hook(block, input) -> None`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`mxnet.gluon.utils.HookHandle`\n",
      " |  \n",
      " |  save_parameters(self, filename)\n",
      " |      Save parameters to file.\n",
      " |      \n",
      " |      Saved parameters can only be loaded with `load_parameters`. Note that this\n",
      " |      method only saves parameters, not model structure. If you want to save\n",
      " |      model structures, please use :py:meth:`HybridBlock.export`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      filename : str\n",
      " |          Path to file.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      `Saving and Loading Gluon Models         <https://mxnet.incubator.apache.org/tutorials/gluon/save_load_params.html>`_\n",
      " |  \n",
      " |  save_params(self, filename)\n",
      " |      [Deprecated] Please use save_parameters. Note that if you want load\n",
      " |      from SymbolBlock later, please use export instead.\n",
      " |      \n",
      " |      Save parameters to file.\n",
      " |      \n",
      " |      filename : str\n",
      " |          Path to file.\n",
      " |  \n",
      " |  summary(self, *inputs)\n",
      " |      Print the summary of the model's output and parameters.\n",
      " |      \n",
      " |      The network must have been initialized, and must not have been hybridized.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      inputs : object\n",
      " |          Any input that the model supports. For any tensor in the input, only\n",
      " |          :class:`mxnet.ndarray.NDArray` is supported.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from mxnet.gluon.block.Block:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  name\n",
      " |      Name of this :py:class:`Block`, without '_' in the end.\n",
      " |  \n",
      " |  params\n",
      " |      Returns this :py:class:`Block`'s parameter dictionary (does not include its\n",
      " |      children's parameters).\n",
      " |  \n",
      " |  prefix\n",
      " |      Prefix of this :py:class:`Block`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f553fef0afcf38327a938fbaf4bf34bdee11e492c9e221e7e73b548254b5a94"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
